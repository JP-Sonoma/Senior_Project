{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to load YOLOv5 onto PYNQ-Z2(FPGA)\n",
    "The goal is to load YOLOv5 by Ultralytics onto the PYNQ-Z2 FPGA, the normal method would be to pip install the Ultralytics library but dependecy clashes prevents this with the PYNQ framework\n",
    "\n",
    "Instead we will attepmt to use the OpenCV Deep Neural Network Library that is already on the PYNQ frame work to run YOLOv5. This was already tested and confired with YOLOv3 to work but now we will try with YOLOv5. This method will run a custom trained version that is exported to a 16 bit floating point ONNX format in an attempt to accerlate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Get to the trained weights (Optional)\n",
    "This step is to be runned if you have git pulled this repository straight away. First check your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to check if in right directory\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not in the weights folder, then run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\perez\\OneDrive\\Documents\\GitHub\\Yolo\n",
      "c:\\Users\\perez\\OneDrive\\Documents\\GitHub\\Yolo\\weights\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "%cd weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be good to begin the rest of the program. If not, then just make sure you are navigated to where you have downloaded the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Set up libraries we need and functions\n",
    "For this part we just need to set up our variables and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start With the Libraries we need such as numpy, opencv, and pynq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # to use the webcam\n",
    "# Additional libraries needed\n",
    "import numpy as np\n",
    "import time\n",
    "# Since we are using the PYNQ board we need the bitstream to access the parts of the board\n",
    "from pynq.overlays.base import BaseOverlay\n",
    "from pynq.lib.video import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the functions to make this whole project to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, net):\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (64, 64), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    preds = net.forward()\n",
    "    return preds\n",
    "\n",
    "def model(path):\n",
    "    net = cv2.dnn.readNetFromONNX(path)\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_DEFAULT)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_BACKEND_DEFAULT)\n",
    "    \n",
    "    return net\n",
    "\n",
    "def format_yolov5(frame):\n",
    "\n",
    "    row, col, _ = frame.shape\n",
    "    _max = max(col, row)\n",
    "    result = np.zeros((_max, _max, 3), np.uint8)\n",
    "    result[0:row, 0:col] = frame\n",
    "    return result\n",
    "\n",
    "def wrap_detection(input_image, output_data):\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    rows = output_data.shape[0]\n",
    "\n",
    "    image_width, image_height, _ = input_image.shape\n",
    "\n",
    "    x_factor = image_width / 64\n",
    "    y_factor =  image_height / 64\n",
    "\n",
    "    for r in range(rows):\n",
    "        row = output_data[r]\n",
    "        confidence = row[4]\n",
    "        if confidence >= 0.4:\n",
    "\n",
    "            classes_scores = row[5:]\n",
    "            _, _, _, max_indx = cv2.minMaxLoc(classes_scores)\n",
    "            class_id = max_indx[1]\n",
    "            if (classes_scores[class_id] > .25):\n",
    "\n",
    "                confidences.append(confidence)\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "                x, y, w, h = row[0].item(), row[1].item(), row[2].item(), row[3].item() \n",
    "                left = int((x - 0.5 * w) * x_factor)\n",
    "                top = int((y - 0.5 * h) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.25, 0.45) \n",
    "\n",
    "    result_class_ids = []\n",
    "    result_confidences = []\n",
    "    result_boxes = []\n",
    "\n",
    "    for i in indexes:\n",
    "        result_confidences.append(confidences[i])\n",
    "        result_class_ids.append(class_ids[i])\n",
    "        result_boxes.append(boxes[i])\n",
    "\n",
    "    return result_class_ids, result_confidences, result_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we need to do some setup for the FPGA specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to setup the bitstream overlay for the PYNQ board\n",
    "base = BaseOverlay(\"base.bit\")\n",
    "\n",
    "# monitor configuration: 640*480 @ 60Hz\n",
    "Mode = VideoMode(640,480,24)\n",
    "hdmi_out = base.video.hdmi_out\n",
    "hdmi_out.configure(Mode,PIXEL_BGR)\n",
    "hdmi_out.start()\n",
    "\n",
    "# monitor (output) frame buffer size\n",
    "frame_out_w = 1920\n",
    "frame_out_h = 1080\n",
    "# camera (input) configuration\n",
    "frame_in_w = 640\n",
    "frame_in_h = 480\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENCV_LOG_LEVEL\"]=\"SILENT\"\n",
    "# initialize camera from OpenCV\n",
    "\n",
    "videoIn = cv2.VideoCapture(0)\n",
    "#videoIn = cv2.cvtColor(cv2.COLOR_BGR2RGB)\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_WIDTH, frame_in_w);\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_in_h);\n",
    "\n",
    "\n",
    "print(\"Capture device is open: \" + str(videoIn.isOpened()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this all worked, then we should see that the capture device is open else check your webcam connection. Now one more function for writing to an HDMI output and finally some additional variable for the whole setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PYNQ_Display(frame_vga):\n",
    "    outframe = hdmi_out.newframe()\n",
    "    outframe[0:480,0:640,:] = frame_vga[0:480,0:640,:]\n",
    "    hdmi_out.writeframe(outframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m colors \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)]\n\u001b[1;32m----> 3\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov5_half_export.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m      6\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mmodel\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel\u001b[39m(path):\n\u001b[1;32m----> 8\u001b[0m     net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mreadNetFromONNX(path)\n\u001b[0;32m      9\u001b[0m     net\u001b[38;5;241m.\u001b[39msetPreferableBackend(cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mDNN_BACKEND_DEFAULT)\n\u001b[0;32m     10\u001b[0m     net\u001b[38;5;241m.\u001b[39msetPreferableTarget(cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mDNN_BACKEND_DEFAULT)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "colors = [(255, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 0)]\n",
    "\n",
    "net = model(\"YOLOv5_three_gestures_int8_12.onnx\")\n",
    "\n",
    "start = time.time_ns()\n",
    "frame_count = 0\n",
    "total_frames = 0\n",
    "fps = -1\n",
    "\n",
    "classes = [] # Gets our Object names\n",
    "with open(\"obj.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Run YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has worked well, then all we have to do is run the yolo code and it should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    ret, frame = videoIn.read()\n",
    "    if frame is None:\n",
    "        print(\"End of stream\")\n",
    "        break\n",
    "\n",
    "    inputImage = format_yolov5(frame)\n",
    "    outs = predict(inputImage, net)\n",
    "\n",
    "    class_ids, confidences, boxes = wrap_detection(inputImage, outs[0])\n",
    "\n",
    "    frame_count += 1\n",
    "    total_frames += 1\n",
    "\n",
    "    for (classid, confidence, box) in zip(class_ids, confidences, boxes):\n",
    "         color = colors[int(classid) % len(colors)]\n",
    "         cv2.rectangle(frame, box, color, 2)\n",
    "         cv2.rectangle(frame, (box[0], box[1] - 20), (box[0] + box[2], box[1]), color, -1)\n",
    "         cv2.putText(frame, classes[classid], (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,0))\n",
    "\n",
    "    if frame_count >= 30:\n",
    "        end = time.time_ns()\n",
    "        fps = 1000000000 * frame_count / (end - start)\n",
    "        frame_count = 0\n",
    "        start = time.time_ns()\n",
    "    \n",
    "    if fps > 0:\n",
    "        fps_label = \"FPS: %.2f\" % fps\n",
    "        cv2.putText(frame, fps_label, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        PYNQ_Display(frame)\n",
    "    \n",
    "    print(\"Total frames: \" + str(total_frames))\n",
    "    if cv2.waitKey(1) > -1:\n",
    "        print(\"finished by user\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoIn.release()\n",
    "cv2.destroyAllWindows()\n",
    "hdmi_out.stop()\n",
    "del hdmi_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
